\chapter{State of the Art}\label{chap:stateArt}

Over the course of this chapter, I am going to present an overview of the most recent developments related to the work of this thesis, both in terms of existing dedicated hardware implementations~\ref{sec:sa_hardware}, the most relevant work in adapting suitable training algorithms to hardware~\ref{sec:sa_training} and also some of the most relevant applications of LSTM~\ref{sec:sa_apps}, which are not FPGA-based, but demonstrate how LSTM are useful by themselves, and how well it competes with other Machine Learning algorithms in terms of long time-series dependences in data.

\section{LSTM Applications (non-FPGA related)}\label{sec:sa_apps} 
LSTM Networks are nowadays one of the state of the art algorithms in deep-learning, and their performance is superior to that of other kinds of RNNs and Hidden Markov Models, both of which are generally used to address the same set of problems where LSTM are employed, namely predicting and producing classification decisions from time-series data. A very comprehensive description of applications can be found in one of the initial authors webpage dedicated to the subject~\footnote{\href{http://people.idsia.ch/~juergen/rnn.html}{http://people.idsia.ch/~juergen/rnn.html}}. I will now enumerate some of the leading edge applications of LSTM.

\begin{itemize}
    \item \textbf{Handwriting Recognition} -- an LSTM-based network~\cite{Bertolami09}, submitted by a team of the Technische Universität München, won the 2009 ICDAR Handwriting Recognition Contest, achieveing a recognition rate of up to 91\%~\cite{ICDAR09}. LSTMs have also proven to surpass HMM-based models in terms of optical character recognition of printed text, as~\cite{Breuel13} suggests.

    \item \textbf{Speech Recognition} -- an architecture\cite{Graves13} proposed by Graves et al. in 2013 achieved an astonishing 17.7\% of accuracy on the TIMT Phoneme Recognition Benchmark, which up to the date is a new record. Furthermore, it has also been used for large scale acoustic modelling of speech~\cite{Sak14}.

    \item \textbf{Handwriting Synthesis} -- a comprehensive study by Graves shows, among other sequence generation tasks such as Text Prediction, that use of LSTM to produce synthetic handwriting, that looks incredibly similar to human-produced handwriting~\cite{Graves13_2}.

    \item \textbf{Translation} -- LSTM was used by Sutskever et al. (Google) to perform sequence-to-sequence translation on the WMT'14 dataset, translating English to French with a close to state of the art score~\cite{Sustkever14}.

    \item \textbf{Biomedical Applications} -- this network architecture was used in a protein homology detection scheme~\cite{Hochreiter07} using the SCOP 1.53 benchmark dataset, displaying a good performance when compared to other methods. Similarly, a recent article from 2015 by Sønderby et al. suggested the use of standalone LSTM and also Convolutional LSTM to perform sub cellular localization of proteins, given solely their protein sequence, with an astounding accuracy of 0.902~\cite{Sonderby15}

    \item \textbf{Music Analysis and Composition} -- Lehner et al. proposed a low-latecy solution based on LSTM, suitable for real-time detection of a singing voice in music recordings~\cite{Lehner15} whose performance surpassed other baseline methods, with lower latency. 
        In terms of music transcription from audio data, there is a study that proposes the use of LSTM cells to perform a transcription of piano recordings to musical notes~\cite{Bock12}, in order to automate music transcription. The model was trained with recordings of both acoustic pianos and synthesized pianos, and the labelling was performed using an associated MIDI file for each piece that was used in the training, showing promising results. \cite{Coca13} suggests the use of LSTM to perform autonomous computer music composition, and Eck and Schmidhüber proposed LSTM to perform Blues improvisation in~\cite{Eck02}.

    \item \textbf{Video and Image Analysis} -- Vinyals et al., at Google, proposed an LSTM network for \textbf{image captioning}, preceded by a Convolutional Neural Network (CNN) to apply preprocessing to the images~\cite{Vinyals14}. The LSTM works as \emph{sentence generator}, that captions the images with state of the art performance. 
        Besides image captioning, video captioning is also an interesting topic. Venugopalan et al. recently proposed a CNN + LSTM architecture to translate video sequences to natural language~\cite{Donahue14}, using the Microsoft Research Video Description Corpus as a dataset.There are other similar studies that combine image and video captioning~\cite{Donahue14_2}.
\end{itemize}

\section{Hardware Implementations of LSTM}\label{sec:sa_hardware}
Hitherto, there is but one actual implementation of an LSTM network in hardware, published recently (November 2015) by Chang et al.~\cite{Chang15} in the Computing Research Repository (CoRR). It consists on the proposal of an LSTM cell architecture for dedicated hardware, targeting a Xilinx\textregistered~ Zedboard implementation. It uses a character-level language model from Andrej Karpathy~\footnote{\href{https://github.com/karpathy/char-rnn}{https://github.com/karpathy/char-rnn}}, written in Lua using the Torch7 framework~\footnote{\href{http://torch.ch/}{http://torch.ch/}} (the Lua calls are implemented in C, so no performance is lost).

Although the article is not clear on whether there is active learning by the ARM CPU -- the authors refer that the CPU loads the weights before operation, and that it changes them during operation, although how and why that change is done is not even clearly explained, neither mathematically nor conceptually --, \textbf{there is no on-chip learning module} in the FPGA according to the description provided. 

The implementation itself makes an extensive use of the Multiply-and-Accumulate units (MAC) of the FPGA, which since they are limited in number in the target platform, limits the number of neurons that we can deploy in parallel. The authors report a nearly 75\% usage for only 8 LSTM neurons. Although an apparently excessive number, I am left to scrutinize whether the usage of MACs is compulsory to obtain a better performance than a CPU or if it can be discarded, allowing for smaller cells. This way, we can have a layer of more neurons occupying the same area and, hopefully, the same, or even fewer, resources within the FPGA.

\section{Training Algorithms and Implementation}\label{sec:sa_training}
As stated in~\ref{sec:sa_hardware}, the work of~\cite{Chang15} does not feature on-chip learning at FPGA level, although there are a handful proposed solutions for it in recent literature. I will particularly look into the ones that use SPSA (see Section~\ref{sec:training_lstm}), since that is the training algorithm of choice for my proposed solution, and also to the ones that particularly apply SPSA to the training of Neural Networks. 

The SPSA algorithm was initially published by Spall in~\cite{Spall98}, and its theoretical details are outlined in Section~\ref{sec:training_lstm}. As to its applications to the training of general Neural Networks, the earliest examples come from 1995 and 1996 in~\cite{Maeda95, Cauwen96} where SPSA is used to train a VLSI \emph{Analog} Neural Networks, a time where the memory resources of digital circuitry were limited, and so most of these structures were analog-based. Its adequacy was also established for \textbf{control problems}, such as those proposed in~\cite{Figueiredo97}, where a Time Delay Neural Network is used to control an unknown plant in a linear feedback system.

In 2005, Maeda and Wakamura published a proposed SPSA hardware implementation~\cite{Maeda05} to train an Hopfield Neural Network in an FPGA (and thus a digital system), achieving promising results in an Altera FPGA. The article carefully delineates the approach taken, and also the hardware architecture designed, so it is a very good reference for the design that I will have to implement. 

Furthermore, a 2013 article by Tavear et al.~\cite{Tavear13} proposes, for the first time, using SPSA to train LSTM Neurons, although the article focuses on proving the suitability of SPSA to LSTM, and no factual hardware implementation is done or proposed. The authors simply demonstrate the suitability using conceptual arguments and by building a software model of an SPSA-trained LSTM network, and by comparing both the performance and computing speed of their model with the results achieved by Hochreiter et al. in~\cite{Hochreiter07}. Since the forward phase in both regular LSTM and SPSA-trained LSTM is the same, the computation time suffers no performance penalty whatsoever and the learning ability is preserved to a high degree, showing that SPSA is a valid alternative do BPTT and other similar and more common training schemes.

\section{Final Overview}\label{sec:overview}
As we could attest from this small literature survey, although there is already an hardware implementation os LSTM, there is still a good deal of room for improvement by adding on chip-learning to the system, and also to restrict it to a smaller use of the FPGA resources, allowing it to accomodate a more complex network. Furthermore,~\cite{Tavear13} shows that, at least for that particular case, the LSTM network doesn't suffer a great performance impact from using SPSA training, as opposed to the more common BPTT, and~\cite{Maeda05} showed that an SPSA hardware implementation is feasible. These three conclusions combined indicate that the idea of a hardware LSTM network with on-chip learning using SPSA is possible. 

Besides the proposed hardware implementation, it would be advantageous to perform \emph{benchmarks} to see how well it compares with software solutions, and Section~\ref{sec:sa_apps} suggests a handful of examples to test my final solution.
