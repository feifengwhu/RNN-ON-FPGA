\chapter{State of the Art}\label{chap:stateArt}

Over the course of this chapter, I am going to present an overview of the most recent developments related to the work of this thesis, both in terms of existing dedicated hardware implementations~\ref{sec:sa_hardware}, the most relevant work in adapting suitable training algorithms to hardware~\ref{sec:sa_training} and also some of the most relevant applications of LSTM~\ref{sec:sa_apps}, which are not FPGA-based, but demonstrate how LSTM is useful by itself, and how well it competes with other Machine Learning algorithms in terms of long time-series dependences in data.

\section{LSTM Applications (non-FPGA related)}\label{sec:sa_apps} 
LSTM Networks are nowadays one of the state of the art algorithms in deep-learning, and their performance is superior to that of other kinds of RNNs and Hidden Markov Models, both of which are generally used to address the same set of problems where LSTM are employed, namely predicting and producing classification decisions from time-series data \textbf{citation needed!}. A very comprehensive description of applications can be found in one of the initial authors webpage dedicated to the subject~\footnote{\href{http://people.idsia.ch/~juergen/rnn.html}{http://people.idsia.ch/~juergen/rnn.html}}. I will now enumerate some of the bleeding edge applications of LSTM.

\begin{itemize}
    \item \textbf{Handwriting Recognition} -- and LSTM-based network~\cite{Bertolami09}, submitted by a team of the Technische Universität München, won the 2009 ICDAR Handwriting Recognition Contest, achieveing a recognition rate of up to 91\%~\cite{ICDAR09}.
    \item \textbf{Speech Recognition} -- an architecture\cite{Graves13} proposed by Graves et al. in 2013 achieved an astonishing 17.7\% of accuracy on the TIMT Phoneme Recognition Benchmark, which up to the date is a new record. Furthermore, it has also been used for large scale acoustic modelling of speech~\cite{Sak14}.
    \item \textbf{Handwriting Synthesis} --
    \item \textbf{Speech Synthesis} --
    \item \textbf{Biomedical Applications} --
    \item \textbf{Music Analysis and Generation} --
    \item \textbf{Video data Analysis} --


\end{itemize}




\section{Hardware Implementations of LSTM}\label{sec:sa_hardware}
Hitherto, there is but one actual implementation of an LSTM network in hardware, published recently (November 2015) by Chang et al.~\cite{Chang15} in the Computing Research Repository (CoRR). It consists on the proposal of an LSTM cell architecture for dedicated hardware, targeting a Xilinx\textregistered~ Zedboard implementation. It uses a character-level language model from Andrej Karpathy, written in Lua using the Torch7 framework (the Lua calls are implemented in C, so no performance is lost).

Although the article is not clear on whether there is active learning by the ARM CPU -- the authors refer that the CPU loads the weights before operation, and that it changes them during operation, although how and why that change is done is not even clearly explained, neither mathematically nor conceptually --, \textbf{there is no on-chip learning module} in the FPGA according to the description provided. 

The implementation itself makes an extensive use of the Multiply-and-Accumulate units (MAC) of the FPGA, which since they are limited in number in the target platform, limits the number of neurons that we can deploy in parallel. The authors report a nearly 75\% usage for only 8 LSTM neurons. Although an apparently excessive number, I am left to scrutinize whether the usage of MACs is compulsory to obtain a better performance than a CPU or if it can be discarded, allowing for smaller cells. This way, we can have a layer of more neurons occupying the same area and, hopefully, the same, or even less, resources within the FPGA.

\section{Training Algorithms and Implementation}\label{sec:sa_training}
As stated in~\ref{sec:sa_hardware}, the work of~\cite{Chang15} does not feature on-chip learning at FPGA level, although there are a handful proposed solutions for it in recent literature. I will particularly look into the ones that use SPSA (see Section~\ref{sec:training_lstm}), since that is the training algorithm of choice for my proposed solution, and also to the ones that particularly apply SPSA to the training of Neural Networks. 

The SPSA algorithm was initially published by Spall in~\cite{Spall98}, and its theoretical details are outlined in Section~\ref{sec:training_lstm}. As of applications of it to the training of general Neural Networks, the earliest examples come from 1995 and 1996 on~\cite{Maeda95, Cauwen96} where SPSA is used to train a VLSI \emph{Analog} Neural Networks, a time where the memory resources of digital circuitry were limited, and so most of these structures were analog-based. Its adequacy was also established for \textbf{control problems}, such as those proposed in~\cite{Figueiredo97}, where a Time Delay Neural Network is used to control an unknown plant in a linear feedback system.

In 2005, Maeda and Wakamura published a proposed SPSA hardware implementation~\cite{Maeda05} to train an Hopfield Neural Network in an FPGA (and thus a digital system), achieving promising results in an Altera \textregistered~ FPGA. The article carefully delineates the approach taken, and also the hardware architecture designed, so it is a very good reference for the design that I will have to implement. 

Furthermore, a 2013 article by Tavear et al.~\cite{Tavear13} proposes, for the first time, using SPSA to train LSTM Neurons, although the article focuses on proving the suitability of SPSA to LSTM, and no factual hardware implementation is done or proposed. The authors simply demonstrate the suitability using conceptual arguments and by building a software model of an SPSA-trained LSTM network, and by comparing both the performance and computing speed of their model with the results achieved by Hochreiter et al. in~\cite{Hochreiter07}. Since the forward phase in both regular LSTM and SPSA-trained LSTM is the same, the computation time suffers no performance penalty whatsoever and the learning ability is preserved to a high degree, showing that SPSA is a valid alternative do BPTT and other similar and more common training schemes.

\section{Final Overview}\label{sec:overview}
