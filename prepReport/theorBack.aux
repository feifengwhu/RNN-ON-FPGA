\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Problem Characterization}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:theorBack}{{2}{3}{Problem Characterization}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Theoretical Background}{3}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Basic Concepts of Machine Learning}{3}{subsection.2.1.1}}
\newlabel{eq:costfunctionFund}{{2.1}{4}{Basic Concepts of Machine Learning}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Artificial Neural Networks}{4}{subsection.2.1.2}}
\newlabel{sec:theorBack_ann}{{2.1.2}{4}{Artificial Neural Networks}{subsection.2.1.2}{}}
\newlabel{eq:gradDesc}{{2.3}{5}{Artificial Neural Networks}{equation.2.1.3}{}}
\newlabel{eq:partialE}{{2.4}{5}{Artificial Neural Networks}{equation.2.1.4}{}}
\citation{Bishop2006}
\newlabel{eq:deltaj}{{2.5}{6}{Artificial Neural Networks}{equation.2.1.5}{}}
\newlabel{eq:update}{{2.6}{6}{Artificial Neural Networks}{equation.2.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Recurrent Neural Networks}{6}{subsection.2.1.3}}
\newlabel{sec:theorBack_rnn}{{2.1.3}{6}{Recurrent Neural Networks}{subsection.2.1.3}{}}
\citation{Bengio1991}
\citation{Yoshua01}
\citation{Yoshua94,Yoshua01}
\citation{Hoch97}
\citation{Gers00}
\citation{Gers2000}
\citation{Graves05}
\citation{Greff15}
\citation{Graves05}
\citation{Graves05}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Long Short-Term Memory Networks}{7}{subsection.2.1.4}}
\newlabel{sec:theorBack_lstm}{{2.1.4}{7}{Long Short-Term Memory Networks}{subsection.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.1}Structure, Operation and Equations}{7}{subsubsection.2.1.4.1}}
\newlabel{sec:struct_lstm}{{2.1.4.1}{7}{Structure, Operation and Equations}{subsubsection.2.1.4.1}{}}
\citation{Greff15}
\citation{Graves05}
\citation{Hoch97}
\citation{Greff15}
\citation{Spall98}
\newlabel{eq:equationsLSTM}{{2.7}{8}{Structure, Operation and Equations}{equation.2.1.7}{}}
\citation{Maeda05}
\citation{Maeda05}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.2}Training -- SPSA}{9}{subsubsection.2.1.4.2}}
\newlabel{sec:training_lstm}{{2.1.4.2}{9}{Training -- SPSA}{subsubsection.2.1.4.2}{}}
\newlabel{eq:spsa_weightUpdate}{{2.13}{9}{Training -- SPSA}{equation.2.1.13}{}}
\newlabel{eq:spsa_updateRule}{{2.14}{9}{Training -- SPSA}{equation.2.1.14}{}}
\newlabel{eq:spsa_gradProof}{{2.15}{9}{Training -- SPSA}{equation.2.1.15}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:modelNeuron_a}{{2.1a}{10}{Neural Network Node\relax }{figure.caption.8}{}}
\newlabel{sub@fig:modelNeuron_a}{{a}{10}{Neural Network Node\relax }{figure.caption.8}{}}
\newlabel{fig:modelNeuron_b}{{2.1b}{10}{Biological Neuron Diagram\relax }{figure.caption.8}{}}
\newlabel{sub@fig:modelNeuron_b}{{b}{10}{Biological Neuron Diagram\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces In Figure~\ref  {fig:modelNeuron_a}. each input feature $x_i$ is weighted by its corresponding weight $w_i$. During the training procedure, these weights are adjusted so that the output $y$ approaches the target value. In Figure~\ref  {fig:modelNeuron_b}, we see the diagram of an actual multi polar neuron. The dendrites, where the stimuli are received, plays a role similar to that of the input nodes. The axon transmits the signal to the synaptic terminals, that are similar to the $y$ output\relax }}{10}{figure.caption.8}}
\newlabel{fig:modelNeuron}{{2.1}{10}{In Figure~\ref {fig:modelNeuron_a}. each input feature $x_i$ is weighted by its corresponding weight $w_i$. During the training procedure, these weights are adjusted so that the output $y$ approaches the target value. In Figure~\ref {fig:modelNeuron_b}, we see the diagram of an actual multi polar neuron. The dendrites, where the stimuli are received, plays a role similar to that of the input nodes. The axon transmits the signal to the synaptic terminals, that are similar to the $y$ output\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Three different activation functions. As you can see, the hyperbolic tangent has the same extreme value as the sign step function, but has a smooth transition between them, which can be interpreted as a \emph  {soft decision} in the more ambiguous middle region, reflecting the degree of uncertainty on the decision. On the other hand, the sigmoid function goes from zero to one, and is also smooth like the hyperbolic tangent\relax }}{10}{figure.caption.9}}
\newlabel{fig:activFunc}{{2.2}{10}{Three different activation functions. As you can see, the hyperbolic tangent has the same extreme value as the sign step function, but has a smooth transition between them, which can be interpreted as a \emph {soft decision} in the more ambiguous middle region, reflecting the degree of uncertainty on the decision. On the other hand, the sigmoid function goes from zero to one, and is also smooth like the hyperbolic tangent\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A three layer ANN. We have omitted some of the connections in the hidden layer, for simplification purposes. $\mathbf  {w}_1$ represents the weight matrix of the input layer, $\mathbf  {w}_2$ the weight matrix of the connections between the input layer and the hidden layer, and $\mathbf  {w}_3$ the weight connections between the hidden and the output layer. $f_{ij}(\dots  )$ is the activation function of the $j$-th neuron of the $i$-th layer. Since they can be different, I chose different indexes to each.\relax }}{11}{figure.caption.10}}
\newlabel{fig:neuralnet}{{2.3}{11}{A three layer ANN. We have omitted some of the connections in the hidden layer, for simplification purposes. $\mb {w}_1$ represents the weight matrix of the input layer, $\mb {w}_2$ the weight matrix of the connections between the input layer and the hidden layer, and $\mb {w}_3$ the weight connections between the hidden and the output layer. $f_{ij}(\dots )$ is the activation function of the $j$-th neuron of the $i$-th layer. Since they can be different, I chose different indexes to each.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A complete LSTM neuron, with all the features as described in~\cite  {Graves05}\relax }}{11}{figure.caption.11}}
\newlabel{fig:lstmneuron}{{2.4}{11}{A complete LSTM neuron, with all the features as described in~\cite {Graves05}\relax }{figure.caption.11}{}}
\@setckpt{theorBack}{
\setcounter{page}{12}
\setcounter{equation}{15}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{subsubsection}{2}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{linenumber}{0}
\setcounter{LN@truepage}{31}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{14}
\setcounter{section@level}{3}
}
