% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{Sak14,
  Title                    = {Long short-term memory recurrent neural network architectures for
 large scale acoustic modeling},
  Author                   = {Hasim Sak and
 Andrew W. Senior and
 Fran{\c{c}}oise Beaufays},
  Booktitle                = {{INTERSPEECH} 2014, 15th Annual Conference of the International Speech
 Communication Association, Singapore, September 14-18, 2014},
  Year                     = {2014},
  Pages                    = {338--342},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/conf/interspeech/SakSB14},
  Crossref                 = {DBLP:conf/interspeech/2014},
  Timestamp                = {Wed, 18 Feb 2015 08:47:28 +0100},
  Url                      = {http://www.isca-speech.org/archive/interspeech_2014/i14_0338.html}
}

@PhdThesis{Bengio1991,
  Title                    = {Artificial Neural Networks and Their Application to Sequence Recognition},
  Author                   = {Bengio, Yoshua},
  School                   = {McGill University},
  Year                     = {1991},

  Address                  = {Montreal, Que., Canada, Canada},
  Note                     = {UMI Order No. GAXNN-72116 (Canadian dissertation)},

  Publisher                = {McGill University}
}

@Article{Yoshua94,
  Title                    = {Learning long-term dependencies with gradient descent is difficult},
  Author                   = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {1994},
  Note                     = {Gradient based learning algorithms;Information latching;Input/output sequences;Learning algorithms;Parametric dynamical system;Recurrent neural network training;Temporal contingencies;},
  Number                   = {2},
  Pages                    = {157 - 166},
  Volume                   = {5},

  Abstract                 = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
  Copyright                = {Compilation and indexing terms, Copyright 2015 Elsevier Inc.},
  File                     = {:tnn-94-gradient.pdf:PDF},
  ISSN                     = {10459227},
  Key                      = {Neural networks},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/72.279181}
}

@Book{Bishop2006,
  Title                    = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  Author                   = {Bishop, Christopher M.},
  Publisher                = {Springer-Verlag New York, Inc.},
  Year                     = {2006},

  Address                  = {Secaucus, NJ, USA},

  ISBN                     = {0387310738}
}

@InProceedings{Bock12,
  Title                    = {Polyphonic piano note transcription with recurrent neural networks},
  Author                   = {Bock, Sebastian and Schedl, Markus},
  Year                     = {2012},

  Address                  = {Kyoto, Japan},
  Pages                    = {121 - 124},

  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {15206149},
  Journal                  = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  Url                      = {http://dx.doi.org/10.1109/ICASSP.2012.6287832}
}

@InProceedings{Breuel13,
  Title                    = {High-Performance OCR for Printed English and Fraktur Using LSTM Networks},
  Author                   = {Breuel, Thomas M. and Ul-Hasan, Adnan and Al-Azawi, Mayce Ali and Shafait, Faisal},
  Booktitle                = {Proceedings of the 2013 12th International Conference on Document Analysis and Recognition},
  Year                     = {2013},

  Address                  = {Washington, DC, USA},
  Pages                    = {683--687},
  Publisher                = {IEEE Computer Society},
  Series                   = {ICDAR '13},

  Acmid                    = {2549524},
  Doi                      = {10.1109/ICDAR.2013.140},
  ISBN                     = {978-0-7695-4999-6},
  Keywords                 = {RNN, LSTM Networks, OCR},
  Numpages                 = {5},
  Url                      = {http://dx.doi.org/10.1109/ICDAR.2013.140}
}

@Article{Cauwen96,
  Title                    = {An analog VLSI recurrent neural network learning a continuous-time trajectory},
  Author                   = {Cauwenberghs, G.},
  Journal                  = {Neural Networks, IEEE Transactions on},
  Year                     = {1996},

  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {346-361},
  Volume                   = {7},

  Doi                      = {10.1109/72.485671},
  ISSN                     = {1045-9227}
}

@Article{Chang15,
  Title                    = {Recurrent Neural Networks Hardware Implementation on {FPGA}},
  Author                   = {Andre Xian Ming Chang and
 Berin Martini and
 Eugenio Culurciello},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1511.05552},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChangMC15},
  File                     = {:/home/josefonseca/Documents/thesis/research/chang5.pdf:PDF},
  Timestamp                = {Tue, 01 Dec 2015 19:22:34 +0100},
  Url                      = {http://arxiv.org/abs/1511.05552}
}

@InProceedings{Coca13,
  Title                    = {Computer-aided music composition with LSTM neural network and chaotic inspiration},
  Author                   = {Coca, Andres E. and Correa, Debora C. and Zhao, Liang},
  Year                     = {2013},

  Address                  = {Dallas, TX, United states},
  Pages                    = {International Neural Network Society (INNS); IEEE Computational Intelligence Society (IEEE-CIS) - },

  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {Proceedings of the International Joint Conference on Neural Networks},
  Key                      = {Neural networks},
  Keywords                 = {Chaotic systems;Complex networks;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/IJCNN.2013.6706747}
}

@Article{Donahue14_2,
  Title                    = {Long-term Recurrent Convolutional Networks for Visual Recognition
 and Description},
  Author                   = {Jeff Donahue and
 Lisa Anne Hendricks and
 Sergio Guadarrama and
 Marcus Rohrbach and
 Subhashini Venugopalan and
 Kate Saenko and
 Trevor Darrell},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1411.4389},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/DonahueHGRVSD14},
  Timestamp                = {Mon, 01 Dec 2014 14:32:13 +0100},
  Url                      = {http://arxiv.org/abs/1411.4389}
}

@InProceedings{Eck02,
  Title                    = {Finding temporal structure in music: Blues improvisation with LSTM recurrent networks},
  Author                   = {Eck, D. and Schmidhuber, J.},
  Year                     = {2002},

  Address                  = {Martigny, Switzerland},
  Pages                    = {747 - 756},
  Volume                   = {2002-January},

  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {Neural Networks for Signal Processing - Proceedings of the IEEE Workshop},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/NNSP.2002.1030094}
}

@InProceedings{Gers2000,
  Title                    = {Recurrent nets that time and count},
  Author                   = {Gers, F.A. and Schmidhuber, J.},
  Year                     = {2000},

  Address                  = {Los Alamitos, CA, USA},
  Note                     = {recurrent neural networks;RNN;timing;counting;time intervals;sequential tasks;motor control;rhythm detection;long short-term memory;LSTM;peephole connections;internal cells;multiplicative gates;discrete time steps;stable sequences;highly-nonlinear precisely-timed spike sequences;},
  Pages                    = {189 - 94},
  Volume                   = {vol.3},

  Abstract                 = {The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks (RNN) can in principle learn to make use of it. We focus on long short-term memory (LSTM) because it usually outperforms other RNN. Surprisingly, LSTM augmented by &ldquo;peephole connections&rdquo; from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars. Without external resets or teacher forcing or loss of performance on tasks reported earlier, our LSTM variant also learns to generate very stable sequences of highly nonlinear, precisely timed spikes. This makes LSTM a promising approach for real-world tasks that require to time and count.},
  Copyright                = {Copyright 2000, IEE},
  Journal                  = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
  Keywords                 = {counting circuits;learning (artificial intelligence);recurrent neural nets;sequences;timing;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/IJCNN.2000.861302}
}

@Article{Gers00,
  Title                    = {Learning to forget: continual prediction with LSTM},
  Author                   = {Gers, F.A. and Schmidhuber, J. and Cummins, F.},
  Journal                  = {Neural Computation},
  Year                     = {2000},
  Note                     = {LSTM;learning algorithms;recurrent neural networks;LSTM networks;continual input streams;forget gate;},
  Number                   = { 10},
  Pages                    = {2451 - 71},
  Volume                   = { 12},

  Abstract                 = {Long short-term memory (Hochreiter &amp; Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive &ldquo;forget gate&rdquo; that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. Ail algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
  Address                  = {USA},
  Copyright                = {Copyright 2000, IEE},
  ISSN                     = {0899-7667},
  Keywords                 = {learning (artificial intelligence);recurrent neural nets;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1162/089976600300015015}
}

@Article{Graves13_2,
  Title                    = {Generating Sequences With Recurrent Neural Networks},
  Author                   = {Alex Graves},
  Journal                  = {CoRR},
  Year                     = {2013},
  Volume                   = {abs/1308.0850},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/Graves13},
  File                     = {:/home/josefonseca/Documents/thesis/research/generatingSeqLSTM.pdf:PDF},
  Timestamp                = {Tue, 10 Dec 2013 12:03:02 +0100},
  Url                      = {http://arxiv.org/abs/1308.0850}
}

@Article{Bertolami09,
  Title                    = {A Novel Connectionist System for Unconstrained Handwriting Recognition},
  Author                   = {Graves, A. and Liwicki, M. and Fernandez, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {855-868},
  Volume                   = {31},

  Doi                      = {10.1109/TPAMI.2008.137},
  ISSN                     = {0162-8828}
}

@InProceedings{Graves13,
  Title                    = {Speech recognition with deep recurrent neural networks},
  Author                   = {Graves, A. and Mohamed, A.-R. and Hinton, G.},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  Year                     = {2013},
  Month                    = {May},
  Pages                    = {6645-6649},

  Doi                      = {10.1109/ICASSP.2013.6638947},
  ISSN                     = {1520-6149}
}

@Article{Graves05,
  Title                    = {Framewise phoneme classification with bidirectional lstm and other neural network architectures},
  Author                   = {Alex Graves and Jürgen Schmidhuber},
  Journal                  = {Neural Networks},
  Year                     = {2005},
  Pages                    = {5--6},

  File                     = {:nn_2005.pdf:PDF}
}

@Article{Greff15,
  Title                    = {{LSTM:} {A} Search Space Odyssey},
  Author                   = {Klaus Greff and Rupesh Kumar Srivastava and Jan Koutn{\'{\i}}k and Bas R. Steunebrink and J{\"{u}}rgen Schmidhuber},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1503.04069},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GreffSKSS15},
  File                     = {:lstm_asearchspaceodyssey.pdf:PDF},
  Timestamp                = {Thu, 09 Apr 2015 11:33:20 +0200},
  Url                      = {http://arxiv.org/abs/1503.04069}
}

@InProceedings{ICDAR09,
  Title                    = {ICDAR 2009 Handwriting Recognition Competition},
  Author                   = {Grosicki, E. and El Abed, H.},
  Booktitle                = {Document Analysis and Recognition, 2009. ICDAR '09. 10th International Conference on},
  Year                     = {2009},
  Month                    = {July},
  Pages                    = {1398-1402},

  Doi                      = {10.1109/ICDAR.2009.184},
  ISSN                     = {1520-5363},
  Keywords                 = {handwriting recognition;natural language processing;text analysis;French written text documents;RIMES-database;handwriting recognition;Databases;Dictionaries;Handwriting recognition;Hidden Markov models;Investments;Postal services;Production;System testing;Text analysis;Training data;Handwriting Recognition Competition;RIMES-Database}
}

@Article{Hirokami05,
  Title                    = {Estimation using simultaneous perturbation stochastic approximation},
  Author                   = {Hirokami, T. and Maeda, Y. and Tsukada, H.},
  Journal                  = {Electrical Engineering in Japan},
  Year                     = {2006},
  Note                     = {simultaneous perturbation stochastic approximation;Kiefer-Wolfowitz stochastic approximation algorithm;parameter estimation algorithm;convergence theorem;identification scheme;},
  Number                   = { 2},
  Pages                    = {30 - 9},
  Volume                   = { 154},

  Abstract                 = {The simultaneous perturbation stochastic approximation (SPSA) is an extension of the Kiefer-Wolfowitz stochastic approximation algorithm. In SPSA, since all parameters are perturbed simultaneously, it is possible to modify parameters with only two measurements of an evaluation function regardless of the dimension of the parameter. We propose a parameter estimation algorithm using the SPSA. A convergence theorem for the proposed algorithm is shown. A simulation result also reveals the feasibility of the identification scheme proposed here.},
  Address                  = {USA},
  Copyright                = {Copyright 2006, The Institution of Engineering and Technology},
  ISSN                     = {0424-7760},
  Keywords                 = {approximation theory;convergence of numerical methods;parameter estimation;perturbation techniques;stochastic processes;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1002/eej.20239}
}

@Article{Yoshua01,
  Title                    = {Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies},
  Author                   = {Sepp Hochreiter and Yoshua Bengio and Paolo Frasconi and Jürgen Schmidhuber},
  Year                     = {2001},

  File                     = {:yoshua01gradient.pdf:PDF},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7321}
}

@Article{Hochreiter07,
  Title                    = {Fast model-based protein homology detection without alignment},
  Author                   = {Hochreiter, Sepp and Heusel, Martin and Obermayer, Klaus},
  Journal                  = {BIOINFORMATICS},
  Year                     = {2007},

  Month                    = {{JUL 15}},
  Number                   = {{14}},
  Pages                    = {{1728-1736}},
  Volume                   = {{23}},

  Doi                      = {{10.1093/bioinformatics/btm247}},
  ISSN                     = {{1367-4803}},
  Orcid-numbers            = {{Hochreiter, Sepp/0000-0001-7449-2528}},
  Unique-id                = {{ISI:000249248300003}}
}

@Article{Hoch97,
  Title                    = {Long short-term memory},
  Author                   = {Hochreiter, S. and Schmidhuber, J.},
  Journal                  = {Neural Computation},
  Year                     = {1997},
  Number                   = { 8},
  Pages                    = {1735 - 80},
  Volume                   = { 9},

  Address                  = {USA},
  Copyright                = {Copyright 1997, IEE},
  File                     = {:Hochreiter97_lstm.pdf:PDF},
  ISSN                     = {0899-7667},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735}
}

@InProceedings{Lehner15,
  Title                    = {A low-latency, real-time-capable singing voice detection method with LSTM recurrent neural networks},
  Author                   = {Lehner, B. and Widmer, G. and Bock, S.},
  Year                     = {2015},

  Address                  = {Piscataway, NJ, USA},
  Pages                    = {21 - 5},

  Copyright                = {Copyright 2016, The Institution of Engineering and Technology},
  File                     = {:/home/josefonseca/Documents/thesis/research/lstmSinging.pdf:PDF},
  Journal                  = {2015 23rd European Signal Processing Conference (EUSIPCO). Proceedings},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/EUSIPCO.2015.7362337}
}

@Article{Figueiredo97,
  Title                    = {Learning rules for neuro-controller via simultaneous perturbation},
  Author                   = {Maeda, Y. and de Figueiredo, Rui J.P.},
  Journal                  = {Neural Networks, IEEE Transactions on},
  Year                     = {1997},
  Number                   = {5},
  Pages                    = {1119-1130},
  Volume                   = {8},

  Doi                      = {10.1109/72.623213},
  ISSN                     = {1045-9227}
}

@Article{Maeda95,
  Title                    = {A learning rule of neural networks via simultaneous perturbation and its hardware implementation},
  Author                   = {Yutaka Maeda and Hiroaki Hirano and Yakichi Kanata},
  Journal                  = {Neural Networks},
  Year                     = {1995},
  Pages                    = {251--259},

  File                     = {:/home/josefonseca/Documents/thesis/research/maeda_etal_neuralnetworks95.pdf:PDF}
}

@Article{Maeda05,
  Title                    = {Simultaneous perturbation learning rule for recurrent neural networks and its FPGA implementation},
  Author                   = {Maeda, Y. and Wakamura, M.},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {2005},
  Note                     = {perturbation learning rule;recurrent neural network;FPGA;dynamic information processing;feedforward neural network;recursive learning scheme;correlation learning;analog learning;oscillatory solution;hardware implementation;Hopfield neural network;field-programmable gate array;},
  Number                   = { 6},
  Pages                    = {1664 - 72},
  Volume                   = { 16},

  Abstract                 = {Recurrent neural networks have interesting properties and can handle dynamic information processing unlike ordinary feedforward neural networks. However, they are generally difficult to use because there is no convenient learning scheme. In this paper, a recursive learning scheme for recurrent neural networks using the simultaneous perturbation method is described. The detailed procedure of the scheme for recurrent neural networks is explained. Unlike ordinary correlation learning, this method is applicable to analog learning and the learning of oscillatory solutions of recurrent neural networks. Moreover, as a typical example of recurrent neural networks, we consider the hardware implementation of Hopfield neural networks using a field-programmable gate array (FPGA). The details of the implementation are described. Two examples of a Hopfield neural network system for analog and oscillatory targets are shown. These results show that the learning scheme proposed here is feasible.},
  Address                  = {USA},
  Copyright                = {Copyright 2005, IEE},
  ISSN                     = {1045-9227},
  Keywords                 = {analogue computer circuits;feedforward neural nets;field programmable gate arrays;learning (artificial intelligence);perturbation techniques;recurrent neural nets;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/TNN.2005.852237}
}

@InProceedings{Sonderby15,
  Title                    = {Convolutional LSTM networks for subcellular localization of proteins},
  Author                   = {Sonderby, Soren Kaae and Sonderby, Casper Kaae and Nielsen, Henrik and Winther, Ole},
  Year                     = {2015},

  Address                  = {Mexico City, Mexico},
  Pages                    = {68 - 80},
  Volume                   = {9199},

  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {03029743},
  Journal                  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Language                 = {English}
}

@InProceedings{Spall98,
  Title                    = {Adaptive stochastic approximation by the simultaneous perturbation method},
  Author                   = {Spall, J.C.},
  Year                     = {1998},

  Address                  = {Piscataway, NJ, USA},
  Note                     = {stochastic approximation;loss functions;stochastic search;Newton-Raphson algorithm;Hessian matrix;iterative method;optimization;root-finding;parameter estimation;adaptive estimation;},
  Pages                    = {3872 - 9},
  Volume                   = {vol.4},

  Abstract                 = {Stochastic approximation (SA) has long been applied for problems of minimizing loss functions or root-finding with noisy input information. As with all stochastic search algorithms, there are adjustable algorithm coefficients that must be specified and that can have a profound effect on algorithm performance. It is known that picking these coefficients according to an SA analogue of the deterministic Newton-Raphson algorithm provides an optimal or near-optimal form of the algorithm. This paper presents a general adaptive SA algorithm that is based on an easy method for estimating the Hessian matrix at each iteration while concurrently estimating the primary parameters of interest. The approach applies in both the gradient-free optimization (Kiefer-Wolfowitz) and root-finding/stochastic gradient-based (Robbins-Monro) settings and is based on the &ldquo;simultaneous perturbation&rdquo; idea introduced previously.},
  Copyright                = {Copyright 1999, IEE},
  Journal                  = {Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171)},
  Keywords                 = {adaptive estimation;approximation theory;Hessian matrices;Newton-Raphson method;optimisation;search problems;stochastic processes;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/CDC.1998.761833}
}

@InProceedings{Sustkever14,
  Title                    = {Sequence to sequence learning with neural networks},
  Author                   = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  Year                     = {2014},

  Address                  = {Montreal, QC, Canada},
  Number                   = {January},
  Pages                    = {3104 - 3112},
  Volume                   = {4},

  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {10495258},
  Journal                  = {Advances in Neural Information Processing Systems},
  Language                 = {English}
}

@Article{Tavear13,
  Title                    = {Transforming the LSTM training algorithm for efficient FPGA -based adaptive control of nonlinear dynamic systems},
  Author                   = {Tavear, R. and Dedic, J. and Bokal, D. and Zemva, A.},
  Journal                  = {Informacije MIDEM},
  Year                     = {2013},
  Number                   = { 2},
  Pages                    = {131 - 8},
  Volume                   = { 43},

  Address                  = {Slovenia},
  Copyright                = {Copyright 2014, The Institution of Engineering and Technology},
  File                     = {:/home/josefonseca/Documents/thesis/research/MIDEM_43(2013)2p131.pdf:PDF},
  ISSN                     = {0352-9045},
  Language                 = {English}
}

@Article{Donahue14,
  Title                    = {Translating Videos to Natural Language Using Deep Recurrent Neural
 Networks},
  Author                   = {Subhashini Venugopalan and
 Huijuan Xu and
 Jeff Donahue and
 Marcus Rohrbach and
 Raymond J. Mooney and
 Kate Saenko},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1412.4729},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/VenugopalanXDRMS14},
  File                     = {:/home/josefonseca/Documents/thesis/research/videocaption.pdf:PDF},
  Timestamp                = {Thu, 01 Jan 2015 19:51:08 +0100},
  Url                      = {http://arxiv.org/abs/1412.4729}
}

@Article{Vinyals14,
  Title                    = {Show and Tell: {A} Neural Image Caption Generator},
  Author                   = {Oriol Vinyals and
 Alexander Toshev and
 Samy Bengio and
 Dumitru Erhan},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1411.4555},

  File                     = {:/home/josefonseca/Documents/thesis/research/showandtell.pdf:PDF},
  Timestamp                = {Mon, 01 Dec 2014 14:32:13 +0100},
  Url                      = {http://arxiv.org/abs/1411.4555}
}

@Article{Werbos90,
  Title                    = {Backpropagation through time: what it does and how to do it},
  Author                   = {Werbos, P.J.},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1990},
  Note                     = {pseudocode;pattern recognition;fault diagnosis;backpropagation;systems identification;neural networks;},
  Number                   = { 10},
  Pages                    = {1550 - 60},
  Volume                   = { 78},

  Abstract                 = {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users.},
  Address                  = {USA},
  Copyright                = {Copyright 1991, IEE},
  File                     = {:Werbos.backprop.pdf:PDF},
  ISSN                     = {0018-9219},
  Keywords                 = {identification;neural nets;pattern recognition;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/5.58337}
}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Vanishing Gradients\;0\;Bishop2006\;Yoshua01\;Yoshua94
\;;
1 ExplicitGroup:Books\;0\;Bishop2006\;;
}

