

@PhdThesis{Bengio1991,
  author =    {Bengio, Yoshua},
  title =     {Artificial Neural Networks and Their Application to Sequence Recognition},
  school =    {McGill University},
  year =      {1991},
  address =   {Montreal, Que., Canada, Canada},
  note =      {UMI Order No. GAXNN-72116 (Canadian dissertation)},
  publisher = {McGill University}
}

@Book{Bishop2006,
  title =     {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  publisher = {Springer-Verlag New York, Inc.},
  year =      {2006},
  author =    {Bishop, Christopher M.},
  address =   {Secaucus, NJ, USA},
  isbn =      {0387310738}
}

@Article{Hoch97,
  author =    {Hochreiter, S. and Schmidhuber, J.},
  title =     {Long short-term memory},
  journal =   {Neural Computation},
  year =      {1997},
  volume =    { 9},
  number =    { 8},
  pages =     {1735 - 80},
  address =   {USA},
  copyright = {Copyright 1997, IEE},
  issn =      {0899-7667},
  language =  {English},
  url =       {http://dx.doi.org/10.1162/neco.1997.9.8.1735}
}

@Article{Werbos90,
  author =    {Werbos, P.J.},
  title =     {Backpropagation through time: what it does and how to do it},
  journal =   {Proceedings of the IEEE},
  year =      {1990},
  volume =    { 78},
  number =    { 10},
  pages =     {1550 - 60},
  note =      {pseudocode;pattern recognition;fault diagnosis;backpropagation;systems identification;neural networks;},
  abstract =  {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users.},
  address =   {USA},
  copyright = {Copyright 1991, IEE},
  issn =      {0018-9219},
  keywords =  {identification;neural nets;pattern recognition;},
  language =  {English},
  url =       {http://dx.doi.org/10.1109/5.58337}
}

@Article{Yoshua01,
  author = {Sepp Hochreiter and Yoshua Bengio and Paolo Frasconi and JÃ¼rgen Schmidhuber},
  title =  {Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies},
  year =   {2001},
  file =   {:yoshua01gradient.pdf:PDF},
  url =    {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7321}
}

@Article{Yoshua94,
  author =    {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  title =     {Learning long-term dependencies with gradient descent is difficult},
  journal =   {IEEE Transactions on Neural Networks},
  year =      {1994},
  volume =    {5},
  number =    {2},
  pages =     {157 - 166},
  note =      {Gradient based learning algorithms;Information latching;Input/output sequences;Learning algorithms;Parametric dynamical system;Recurrent neural network training;Temporal contingencies;},
  abstract =  {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
  copyright = {Compilation and indexing terms, Copyright 2015 Elsevier Inc.},
  file =      {:tnn-94-gradient.pdf:PDF},
  issn =      {10459227},
  key =       {Neural networks},
  keywords =  {Algorithms;Binary sequences;Computational complexity;Data storage equipment;Discrete time control systems;Information retrieval systems;Input output programs;Learning systems;Parameter estimation;},
  language =  {English},
  url =       {http://dx.doi.org/10.1109/72.279181}
}