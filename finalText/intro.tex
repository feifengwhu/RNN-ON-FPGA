\chapter{Introduction}\label{chap:intro}

In this work, a hardware implementation of an LSTM Neural Network is presented, along with a proposal for a digital circuit that performs on-chip training.
In the reference example, the proposed hardware network was 251 times faster than the reference software implementation running in a high-end Desktop.
The training convergence, although simulated, was not conclusive.

\section{Background}\label{sec:intro_back}
Artificial Neural Networks are one of the most popular algorithms in the field of Machine Learning. As the name suggests, their operation is inspired by
the operation of the building blocks of our brain, the \textbf{neurons}. In spite of its high performance, one of their shortcomings is the fact that
they cannot retain temporal dependences in a stream of data samples, thus not being suitable to process time-series data, such as audio, video or other
kinds of time-varying data streams, where inputs at a given time step have a high temporal dependence with previous and future inputs.

To address this issue, several algorithms have been used, such as Hidden Markov Models (HMM) or Recurrent Neural Networks (RNN), but both of these methods fail
to recall temporal dependences that extend over a large period of time, for the reasons that we will understand in Sections~\ref{sec:theorBack_rnn} and~\ref{sec:theorBack_lstm}.
In 1997 Hochreiter et al. proposed a novel RNN structure~\cite{Hoch97}, the Long Short-Term Network (LSTM), where a memory cell was introduced, and
the input/output/read/write access is controlled by individual gates that are activated both by the incoming data samples, but also by the outputs
from the previous time-step (it is an RNN after all). They are one of the state of the art methods in Deep Learning nowadays, as we can attest in Section~\ref{sec:sa_apps}.

\section{Motivation}\label{sec:intro_mot}
Hitherto, and to the best of my knowledge, all of the published applications that use LSTM are software based. But the parallel nature of the
structure hollers for a dedicated hardware realization that can dramatically increase its performance, something that has only recently been done
once~\cite{Chang15} (see Section~\ref{sec:sa_hardware} for further details), and although it improves the processing time when compared to a naïve
software solution, it makes use of external memory to store the network parameters, which undermines the network performance. Furthermore, the
design is not flexible to accommodate networks of different sizes, and lacks the ability to perform on-chip learning.

All these techniques are generally implemented in mainstream processors, making use of general high-level or low-level languages, where all the real
parallelism is limited to the number of simultaneous threads that we can run on each physical core, which up to now generally have between 2-8 cores
(mobile devices and general use personal computers),

In order to parallelize the computations to the fullest extent, a solution is to port it both to a Graphics Processing Unit (GPU) or even a
Field-Programmable Gate Array (FPGA), but the porting process is not entirely automatic and to have the least performance drop possible, it has
to be explicitly programmed in CUDA/OpenCL for GPUs, and a Hardware Description Language for FPGAs, with an increasing level of complexity and
low-level details. Therefore, it is necessary to provide frameworks that can allow an FPGA to quickly reconfigure itself to run these kind of
networks on demand, for a particular task that requires them, achieving a lower computation time, and unburdening the CPU from running it,
thus saving performance for running other tasks related with the Operating System, for instance. Furthermore, when processing an incoming stream
of highly dimensional data, or with high throughput, a CPU solution might not be scalable, and could benefit greatly from a dedicated hardware implementation.

\section{Objectives}\label{sec:intro_obj}
Taking into account the considerations done in~\ref{sec:intro_mot}, a hardware implementation of an
LSTM Network is proposed, \emph{with on-chip learning}, improving the performance, capability and flexibility of the existing solution of~\cite{Chang15}.
Moreover, the HDL description of the network is parameterizable, meaning that to synthesize a network for a given size, we only need to
change the appropriate parameter at synthesis time, therefore requiring no redesign effort whatsoever, unlike~\cite{Chang15}.

\section{People Involved}\label{sec:intro_people}
Besides myself, the candidate to the Master's Degree, there are two more people involved, namely

\begin{itemize}
    \item \textbf{Supervisor} -- The \href{https://sigarra.up.pt/feup/pt/func\_geral.formview?p\_codigo=210963}{Professor João Canas Ferreira}, auxiliary professor at the Faculty of Engineering of the University of Porto.
    \item \textbf{Second Supervisor} -- \href{http://www.cl.cam.ac.uk/~ijpdmt2/}{Ivo Timóteo}, MSc, a Computer Science PhD candidate at Cambridge University, UK, in the field of Artificial Intelligence.
\end{itemize}

\section{Overview of the Document}\label{sec:intro_overview}
In Chapter~\ref{chap:theorBack}, the theoretical foundations will be laid out. Section~\ref{sec:tb_basicCon} presents the
basic concepts of Machine Learning, and in the following sections, the theoretical details of ANNs, RNNs and LSTMs.
Section~\ref{sec:training_lstm} provides a quick explanation of the training algorithm that will be used in the final
solution, which will be outlined in Section~\ref{sec:train-circ}.

In Chapter~\ref{chap:stateArt}, the state of the art of LSTMs, their applications (Section~\ref{sec:sa_apps}), their
hardware implementations (Section~\ref{sec:sa_hardware}) and the current work regarding the chosen training algorithm (Section~\ref{sec:sa_training})
are presented.

In Chapter~\ref{chap:propSol} the proposed architecture of the hardware LSTM network is explained in detail, as well as details
related to each of its constituent blocks.

Finally, in Chapter~\ref{chap:results}, I present the results achieved with the hardware network of Chapter~\ref{chap:propSol}.
