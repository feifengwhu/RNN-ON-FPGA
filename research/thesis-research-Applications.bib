Automatically generated by Mendeley Desktop 1.15.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Wang2015,
abstract = {In this paper, we introduce Long Short-Term Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function. Experiments on a public noisy labelled data show that our model outperforms several feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture doubles the performance of non-neural model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases. {\&}copy; 2015 Association for Computational Linguistics.},
author = {Wang, Xin and Liu, Yuanchao and Sun, Chengjie and Wang, Baoxun and Wang, Xiaolong},
journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
pages = {1343--1353},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Predicting polarities of tweets by composing word embeddings with long short-Term memory}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}594c68f01509076a465M562a10178163171{\&}database=cpx},
volume = {1},
year = {2015}
}
@article{Yao2014,
abstract = {Neural network based approaches have recently produced record-setting performances in natural language understanding tasks such as word labeling. In the word labeling task, a tagger is used to assign a label to each word in an input sequence. Specifically, simple recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have shown to significantly outperform the previous state-of-the-art - conditional random fields (CRFs). This paper investigates using long short-term memory (LSTM) neural networks, which contain input, output and forgetting gates and are more advanced than simple RNN, for the word labeling task. To explicitly model output-label dependence, we propose a regression model on top of the LSTM un-normalized scores. We also propose to apply deep LSTM to the task. We investigated the relative importance of each gate in the LSTM by setting other gates to a constant and only learning particular gates. Experiments on the ATIS dataset validated the effectiveness of the proposed models. {\&}copy; 2014 IEEE.},
author = {Yao, Kaisheng and Peng, Baolin and Zhang, Yu and Yu, Dong and Zweig, Geoffrey and Shi, Yangyang},
doi = {10.1109/SLT.2014.7078572},
journal = {2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings},
month = {apr},
pages = {189--194},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Spoken language understanding using long short-term memory neural networks}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}M15808ef714d300bc1b6M56c210178163171{\&}database=cpx},
year = {2014}
}
@article{Landsiedel2011,
abstract = {Segmentation of speech signals is a crucial task in many types of speech analysis. We present a novel approach at segmentation on a syllable level, using a Bidirectional Long-Short-Term Memory Neural Network. It performs estimation of syllable nucleus positions based on regression of perceptually motivated input features to a smooth target function. Peak selection is performed to attain valid nuclei positions. Performance of the model is evaluated on the levels of both syllables and the vowel segments making up the syllable nuclei. The general applicability of the approach is illustrated by good results for two common databases-Switchboard and TIMIT-for both read and spontaneous speech, and a favourable comparison with other published results.},
author = {Landsiedel, C. and Edlund, J. and Eyben, F. and Neiberg, D. and Schuller, B.},
journal = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
publisher = {IEEE, Piscataway, NJ, USA},
title = {{Syllabification of conversational speech using bidirectional long-short-term memory neural networks}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}ef55021323fe8cd56M773f2061377553{\&}database=ins},
year = {2011}
}
@article{Alsharif2015,
abstract = {Gesture typing is an efficient input method for phones and tablets using continuous traces created by a pointed object (e.g., finger or stylus). Translating such continuous gestures into textual input is a challenging task as gesture inputs exhibit many features found in speech and handwriting such as high variability, co-articulation and elision. In this work, we address these challenges with a hybrid approach, combining a variant of recurrent networks, namely Long Short Term Memories [1] with conventional Finite State Transducer decoding [2]. Results using our approach show considerable improvement relative to a baseline shape-matching-based system, amounting to 4{\%} and 22{\%} absolute improvement respectively for small and large lexicon decoding on real datasets and 2{\%} on a synthetic large scale dataset.},
author = {Alsharif, O. and Ouyang, T. and Beaufays, F. and {Shumin Zhai} and Breuel, T. and Schalkwyk, J.},
doi = {10.1109/ICASSP.2015.7178336},
journal = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings},
publisher = {IEEE, Piscataway, NJ, USA},
title = {{Long short term memory neural network for keyboard gesture decoding}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}6a91eda514f479c0c3fM494710178163171{\&}database=ins},
year = {2015}
}
@article{Ma2015,
abstract = {Neural networks have been extensively applied to short-term traffic prediction in the past years. This study proposes a novel architecture of neural networks, Long Short-Term Neural Network (LSTM NN), to capture nonlinear traffic dynamic in an effective manner. The LSTM NN can overcome the issue of back-propagated error decay through memory blocks, and thus exhibits the superior capability for time series prediction with long temporal dependency. In addition, the LSTM NN can automatically determine the optimal time lags. To validate the effectiveness of LSTM NN, travel speed data from traffic microwave detectors in Beijing are used for model training and testing. A comparison with different topologies of dynamic neural networks as well as other prevailing parametric and nonparametric algorithms suggests that LSTM NN can achieve the best prediction performance in terms of both accuracy and stability. {\&}copy; 2015 Elsevier Ltd.},
author = {Ma, Xiaolei and Tao, Zhimin and Wang, Yinhai and Yu, Haiyang and Wang, Yunpeng},
doi = {10.1016/j.trc.2015.03.014},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
month = {may},
pages = {187--197},
publisher = {Elsevier Ltd},
title = {{Long short-term memory neural network for traffic speed prediction using remote microwave sensor data}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}2e511c8214db4bb3408M613c10178163171{\&}database=cpx},
volume = {54},
year = {2015}
}
