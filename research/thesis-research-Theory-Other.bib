Automatically generated by Mendeley Desktop 1.15.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Tavear2013,
abstract = {In the absence of high-fidelity analytical descriptions of a given system to be modeled, designers of model-driven control systems rely on empirical nonlinear modeling methods such as neural networks. The particularly challenging task of modeling time-varying nonlinear dynamic systems requires from the modeling technique to capture complex internal system dynamics, dependent of long input histories. Traditional recurrent neural networks (RNNs) can in principle satisfy these demands, but have limitations on retaining long-term input data. Long Short-Term Memory (LSTM) neural networks overcome these limitations. In applications with strict requirements imposed on the size, power consumption and speed, embedded implementations of control systems based on Field Programmable Gate Array (FPGA) technology are required. However, as neural networks are traditionally a software discipline, direct ports of neural networks and their learning algorithms into hardware give disappointing, often impractical results. To enable efficient hardware implementation of LSTM with on-chip learning, we present a transformation strategy which leads to replacing original LSTM learning algorithm with Simultaneous Perturbation Stochastic Approximation (SPSA). Our experimental results on a protein sequence classification benchmark confirm the efficacy of the presented learning scheme. The use of this scheme streamlines the architecture of on-chip learning phase substantially and enables efficient implementation of both forward phase and learning phase in FPGA based hardware.},
author = {Tavear, R. and Dedic, J. and Bokal, D. and Zemva, A.},
issn = {0352-9045},
journal = {Informacije MIDEM},
month = {jun},
number = {2},
publisher = {Elektrotehniska zveza Slovenije, Slovenia},
title = {{Transforming the LSTM training algorithm for efficient FPGA -based adaptive control of nonlinear dynamic systems}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}5de25bd9145cde170deM7c7210178163125{\&}database=ins},
volume = {43},
year = {2013}
}
@article{Bakker2002,
abstract = {This paper presents reinforcement learning with a Long ShortTerm Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage({\&}lambda;) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task.},
author = {Bakker, Bram},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
publisher = {Neural information processing systems foundation},
title = {{Reinforcement learning with long short-term memory}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}3f4b8886145d8e63e07M724510178163125{\&}database=cpx},
year = {2002}
}
@article{Sainath2015,
abstract = {Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) have shown improvements over Deep Neural Networks (DNNs) across a wide variety of speech recognition tasks. CNNs, LSTMs and DNNs are complementary in their modeling capabilities, as CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and DNNs are appropriate for mapping features to a more separable space. In this paper, we take advantage of the complementarity of CNNs, LSTMs and DNNs by combining them into one unified architecture. We explore the proposed architecture, which we call CLDNN, on a variety of large vocabulary tasks, varying from 200 to 2,000 hours. We find that the CLDNN provides a 4-6{\%} relative improvement in WER over an LSTM, the strongest of the three individual models.},
author = {Sainath, T.N. and Vinyals, O. and Senior, A. and Sak, H.},
doi = {10.1109/ICASSP.2015.7178838},
journal = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings},
publisher = {IEEE, Piscataway, NJ, USA},
title = {{Convolutional, Long Short-term Memory, fully connected Deep Neural Networks}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}M5fbfc34514f6f6ccf2dM55f410178163171{\&}database=ins},
year = {2015}
}
