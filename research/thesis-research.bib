Automatically generated by Mendeley Desktop 1.15.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Tavear2013,
abstract = {In the absence of high-fidelity analytical descriptions of a given system to be modeled, designers of model-driven control systems rely on empirical nonlinear modeling methods such as neural networks. The particularly challenging task of modeling time-varying nonlinear dynamic systems requires from the modeling technique to capture complex internal system dynamics, dependent of long input histories. Traditional recurrent neural networks (RNNs) can in principle satisfy these demands, but have limitations on retaining long-term input data. Long Short-Term Memory (LSTM) neural networks overcome these limitations. In applications with strict requirements imposed on the size, power consumption and speed, embedded implementations of control systems based on Field Programmable Gate Array (FPGA) technology are required. However, as neural networks are traditionally a software discipline, direct ports of neural networks and their learning algorithms into hardware give disappointing, often impractical results. To enable efficient hardware implementation of LSTM with on-chip learning, we present a transformation strategy which leads to replacing original LSTM learning algorithm with Simultaneous Perturbation Stochastic Approximation (SPSA). Our experimental results on a protein sequence classification benchmark confirm the efficacy of the presented learning scheme. The use of this scheme streamlines the architecture of on-chip learning phase substantially and enables efficient implementation of both forward phase and learning phase in FPGA based hardware.},
author = {Tavear, R. and Dedic, J. and Bokal, D. and Zemva, A.},
issn = {0352-9045},
journal = {Informacije MIDEM},
month = {jun},
number = {2},
publisher = {Elektrotehniska zveza Slovenije, Slovenia},
title = {{Transforming the LSTM training algorithm for efficient FPGA -based adaptive control of nonlinear dynamic systems}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}5de25bd9145cde170deM7c7210178163125{\&}database=ins},
volume = {43},
year = {2013}
}
@article{Wang2015,
abstract = {In this paper, we introduce Long Short-Term Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function. Experiments on a public noisy labelled data show that our model outperforms several feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture doubles the performance of non-neural model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases. {\&}copy; 2015 Association for Computational Linguistics.},
author = {Wang, Xin and Liu, Yuanchao and Sun, Chengjie and Wang, Baoxun and Wang, Xiaolong},
journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
pages = {1343--1353},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Predicting polarities of tweets by composing word embeddings with long short-Term memory}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}594c68f01509076a465M562a10178163171{\&}database=cpx},
volume = {1},
year = {2015}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued and noisy pattern representations. In comparisons with real-time recurrent learning, backpropagation through time, recurrent cascade correlation, Elman nets and neural sequence chunking, LSTM leads to many more successful runs and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, S. and Schmidhuber, J.},
doi = {10.1162/neco.1997.9.8.1735},
file = {:C$\backslash$:/Users/josefonseca/Downloads/Hochreiter97{\_}lstm.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
publisher = {MIT Press, USA},
title = {{Long short-term memory}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}base905736179{\&}database=ins},
volume = {9},
year = {1997}
}
@article{Bakker2002,
abstract = {This paper presents reinforcement learning with a Long ShortTerm Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage({\&}lambda;) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task.},
author = {Bakker, Bram},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
publisher = {Neural information processing systems foundation},
title = {{Reinforcement learning with long short-term memory}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}3f4b8886145d8e63e07M724510178163125{\&}database=cpx},
year = {2002}
}
@article{Schmidhuber2007,
abstract = {In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks. Sometimes, however, gradient information is of little use for training RNNs, due to numerous local minima. For such cases, we present a novel method: evolution of systems with linear outputs (Evolino). Evolino evolves weights to the nonlinear, hidden nodes of RNNs while computing optimal linear mappings from hidden state to output, using methods such as pseudoinverse-based linear regression. If we instead use quadratic programming to maximize the margin, we obtain the first evolutionary recurrent support vector machines. We show that Evolino-based LSTM can solve tasks that Echo State nets (Jaeger, 2004a) cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM.},
author = {Schmidhuber, J. and Wierstra, D. and Gagliolo, M. and Gomez, F.},
doi = {10.1162/neco.2007.19.3.757},
file = {:C$\backslash$:/Users/josefonseca/Downloads/nc2007.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {mar},
number = {3},
publisher = {MIT Press, USA},
title = {{Training recurrent networks by Evolino}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}13f30451126705eb75M66982061377553{\&}database=ins},
volume = {19},
year = {2007}
}
@article{Graves2005,
abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.11An abbreviated version of some portions of this article appeared in (Graves and Schmidhuber, 2005), as part of the IJCNN 2005 conference proceedings, published under the IEEE copyright. [All rights reserved Elsevier].},
author = {Graves, A. and Schmidhuber, J.},
doi = {10.1016/j.neunet.2005.06.042},
file = {:C$\backslash$:/Users/josefonseca/Downloads/nn{\_}2005.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
number = {5-6},
publisher = {Elsevier, UK},
title = {{Framewise phoneme classification with bidirectional LSTM and other neural network architectures}},
volume = {18},
year = {2005}
}
@article{DBLP:journals/corr/GreffSKSS15,
archivePrefix = {arXiv},
arxivId = {1503.04069v1},
author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'{\i}}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"{u}}rgen},
eprint = {1503.04069v1},
file = {:C$\backslash$:/Users/josefonseca/Downloads/1503.04069v1.pdf:pdf},
journal = {CoRR},
title = {{LSTM: A Search Space Odyssey}},
url = {http://arxiv.org/abs/1503.04069},
volume = {abs/1503.0},
year = {2015}
}
@article{Yao2014,
abstract = {Neural network based approaches have recently produced record-setting performances in natural language understanding tasks such as word labeling. In the word labeling task, a tagger is used to assign a label to each word in an input sequence. Specifically, simple recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have shown to significantly outperform the previous state-of-the-art - conditional random fields (CRFs). This paper investigates using long short-term memory (LSTM) neural networks, which contain input, output and forgetting gates and are more advanced than simple RNN, for the word labeling task. To explicitly model output-label dependence, we propose a regression model on top of the LSTM un-normalized scores. We also propose to apply deep LSTM to the task. We investigated the relative importance of each gate in the LSTM by setting other gates to a constant and only learning particular gates. Experiments on the ATIS dataset validated the effectiveness of the proposed models. {\&}copy; 2014 IEEE.},
author = {Yao, Kaisheng and Peng, Baolin and Zhang, Yu and Yu, Dong and Zweig, Geoffrey and Shi, Yangyang},
doi = {10.1109/SLT.2014.7078572},
journal = {2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings},
month = {apr},
pages = {189--194},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Spoken language understanding using long short-term memory neural networks}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}M15808ef714d300bc1b6M56c210178163171{\&}database=cpx},
year = {2014}
}
@article{Sainath2015,
abstract = {Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) have shown improvements over Deep Neural Networks (DNNs) across a wide variety of speech recognition tasks. CNNs, LSTMs and DNNs are complementary in their modeling capabilities, as CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and DNNs are appropriate for mapping features to a more separable space. In this paper, we take advantage of the complementarity of CNNs, LSTMs and DNNs by combining them into one unified architecture. We explore the proposed architecture, which we call CLDNN, on a variety of large vocabulary tasks, varying from 200 to 2,000 hours. We find that the CLDNN provides a 4-6{\%} relative improvement in WER over an LSTM, the strongest of the three individual models.},
author = {Sainath, T.N. and Vinyals, O. and Senior, A. and Sak, H.},
doi = {10.1109/ICASSP.2015.7178838},
journal = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings},
publisher = {IEEE, Piscataway, NJ, USA},
title = {{Convolutional, Long Short-term Memory, fully connected Deep Neural Networks}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}M5fbfc34514f6f6ccf2dM55f410178163171{\&}database=ins},
year = {2015}
}
@article{Landsiedel2011,
abstract = {Segmentation of speech signals is a crucial task in many types of speech analysis. We present a novel approach at segmentation on a syllable level, using a Bidirectional Long-Short-Term Memory Neural Network. It performs estimation of syllable nucleus positions based on regression of perceptually motivated input features to a smooth target function. Peak selection is performed to attain valid nuclei positions. Performance of the model is evaluated on the levels of both syllables and the vowel segments making up the syllable nuclei. The general applicability of the approach is illustrated by good results for two common databases-Switchboard and TIMIT-for both read and spontaneous speech, and a favourable comparison with other published results.},
author = {Landsiedel, C. and Edlund, J. and Eyben, F. and Neiberg, D. and Schuller, B.},
journal = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
publisher = {IEEE, Piscataway, NJ, USA},
title = {{Syllabification of conversational speech using bidirectional long-short-term memory neural networks}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}ef55021323fe8cd56M773f2061377553{\&}database=ins},
year = {2011}
}
@article{Alsharif2015,
abstract = {Gesture typing is an efficient input method for phones and tablets using continuous traces created by a pointed object (e.g., finger or stylus). Translating such continuous gestures into textual input is a challenging task as gesture inputs exhibit many features found in speech and handwriting such as high variability, co-articulation and elision. In this work, we address these challenges with a hybrid approach, combining a variant of recurrent networks, namely Long Short Term Memories [1] with conventional Finite State Transducer decoding [2]. Results using our approach show considerable improvement relative to a baseline shape-matching-based system, amounting to 4{\%} and 22{\%} absolute improvement respectively for small and large lexicon decoding on real datasets and 2{\%} on a synthetic large scale dataset.},
author = {Alsharif, O. and Ouyang, T. and Beaufays, F. and {Shumin Zhai} and Breuel, T. and Schalkwyk, J.},
doi = {10.1109/ICASSP.2015.7178336},
journal = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings},
publisher = {IEEE, Piscataway, NJ, USA},
title = {{Long short term memory neural network for keyboard gesture decoding}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}6a91eda514f479c0c3fM494710178163171{\&}database=ins},
year = {2015}
}
@article{Ma2015,
abstract = {Neural networks have been extensively applied to short-term traffic prediction in the past years. This study proposes a novel architecture of neural networks, Long Short-Term Neural Network (LSTM NN), to capture nonlinear traffic dynamic in an effective manner. The LSTM NN can overcome the issue of back-propagated error decay through memory blocks, and thus exhibits the superior capability for time series prediction with long temporal dependency. In addition, the LSTM NN can automatically determine the optimal time lags. To validate the effectiveness of LSTM NN, travel speed data from traffic microwave detectors in Beijing are used for model training and testing. A comparison with different topologies of dynamic neural networks as well as other prevailing parametric and nonparametric algorithms suggests that LSTM NN can achieve the best prediction performance in terms of both accuracy and stability. {\&}copy; 2015 Elsevier Ltd.},
author = {Ma, Xiaolei and Tao, Zhimin and Wang, Yinhai and Yu, Haiyang and Wang, Yunpeng},
doi = {10.1016/j.trc.2015.03.014},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
month = {may},
pages = {187--197},
publisher = {Elsevier Ltd},
title = {{Long short-term memory neural network for traffic speed prediction using remote microwave sensor data}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}2e511c8214db4bb3408M613c10178163171{\&}database=cpx},
volume = {54},
year = {2015}
}
